{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-25T03:13:27.390989Z",
     "start_time": "2025-11-25T03:13:27.388776Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T03:13:27.398087Z",
     "start_time": "2025-11-25T03:13:27.394029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# ORIGINAL TRANSFORMER (Attention is All You Need, 2017)\n",
    "# ============================================================================\n",
    "\n",
    "class OriginalMultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention from original Transformer paper\"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        # super(OriginalMultiHeadAttention, self).__init__()\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear projections and split into heads\n",
    "        # (batch_size, seq_len, d_model) -> (batch, num_heads, seq_len, d_k)\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # Concatenate heads and apply output projection\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "\n",
    "        return output"
   ],
   "id": "e5a6b150451b8647",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T03:13:27.403987Z",
     "start_time": "2025-11-25T03:13:27.400966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class OriginalTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Original Transformer Block with POST-NORM architecture\n",
    "    Structure: X -> MHA -> Add -> Norm -> FFN -> Add -> Norm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        # super(OriginalTransformerBlock, self).__init__()\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.attention = OriginalMultiHeadAttention(d_model, num_heads, dropout)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "        # Layer normalization (applied AFTER residual)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with POST-NORM\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        x = x + self.dropout1(attn_output)\n",
    "        x = self.norm1(x)  # Norm AFTER residual\n",
    "\n",
    "        # Feed-forward with POST-NORM\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout2(ffn_output)\n",
    "        x = self.norm2(x)  # Norm AFTER residual\n",
    "\n",
    "        return x"
   ],
   "id": "f42382d2b2f48bbd",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T03:13:27.410306Z",
     "start_time": "2025-11-25T03:13:27.406603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# MODERN TRANSFORMER (Pre-Norm + Improvements)\n",
    "# ============================================================================\n",
    "\n",
    "class ModernMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Modern Multi-Head Attention with optimizations:\n",
    "    - Fused QKV projection for efficiency\n",
    "    - Optional Flash Attention pattern\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1, bias=True):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Fused QKV projection (more efficient)\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=bias)\n",
    "\n",
    "        # Output projection\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        # Fused QKV projection and split\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.d_k)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch, heads, seq_len, d_k)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().reshape(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "\n",
    "        # Output projeciton\n",
    "        output = self.proj(attn_output)\n",
    "        output = self.proj_dropout(output)\n",
    "\n",
    "        return output"
   ],
   "id": "1d268c3363c07ef8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T03:13:27.416510Z",
     "start_time": "2025-11-25T03:13:27.413417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ModernTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Modern Transformer Block with PRE-NORM architecture\n",
    "    Structure: X -> Norm -> MHA -> Add -> Norm -> FFN -> Add\n",
    "\n",
    "    Key improvements:\n",
    "    - Pre-LayerNorm (btter gradient flow, easier training)\n",
    "    - GELU activation (smoother gradients than ReLU)\n",
    "    - Optional bias=False for better performance\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.attention = ModernMultiHeadAttention(d_model, num_heads, dropout, bias=bias)\n",
    "\n",
    "        # Feed-forward network with GELU\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=bias),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model, bias=bias),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Layer normalization (applied BEFORE sublayer)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with PRE-NORM\n",
    "        x = x + self.attention(self.norm1(x), mask)  # Norm BEFORE attention\n",
    "\n",
    "        # Feed-forward with PRE-NORM\n",
    "        x = x + self.ffn(self.norm2(x))  # Norm BEFORE FFN\n",
    "\n",
    "        return x"
   ],
   "id": "b421f71ceb78c028",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T03:13:27.457832Z",
     "start_time": "2025-11-25T03:13:27.423135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# DEMONSTRATION AND COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    dropout = 0.1\n",
    "\n",
    "    # Create sample input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TRANSFORMER BLOCK COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Original Transformer\n",
    "    print(\"\\n1. ORIGINAL TRANSFORMER (Post-Norm, ReLU)\")\n",
    "    print(\"-\" * 70)\n",
    "    original_block = OriginalTransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "    original_output = original_block(x)\n",
    "    print(f\"Input shape:  {x.shape}\")\n",
    "    print(f\"Output shape: {original_output.shape}\")\n",
    "    print(f\"Parameters:   {sum(p.numel() for p in original_block.parameters()):,}\")\n",
    "    print(f\"Architecture: X -> MHA -> Add -> Norm -> FFN -> Add -> Norm\")\n",
    "\n",
    "    # Modern Transformer\n",
    "    print(\"\\n2. MODERN TRANSFORMER (Pre-Norm, GELU)\")\n",
    "    print(\"-\" * 70)\n",
    "    modern_block = ModernTransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "    modern_output = modern_block(x)\n",
    "    print(f\"Input shape:  {x.shape}\")\n",
    "    print(f\"Output shape: {modern_output.shape}\")\n",
    "    print(f\"Parameters:   {sum(p.numel() for p in modern_block.parameters()):,}\")\n",
    "    print(f\"Architecture: X -> Norm -> MHA -> Add -> Norm -> FFN -> Add\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"KEY DIFFERENCES\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\"\"\n",
    "Original (Post-Norm):\n",
    "  ✓ Residual connection first, then normalization\n",
    "  ✓ ReLU activation in FFN\n",
    "  ✓ Separate Q, K, V projections\n",
    "  ✗ Can have training instability with deep networks\n",
    "  ✗ Requires careful learning rate tuning\n",
    "\n",
    "Modern (Pre-Norm):\n",
    "  ✓ Normalization first, then residual connection\n",
    "  ✓ GELU activation (smoother gradients)\n",
    "  ✓ Fused QKV projection (more efficient)\n",
    "  ✓ Better gradient flow, easier to train deep networks\n",
    "  ✓ More stable training, works well with larger learning rates\n",
    "  ✓ Used in GPT-2, GPT-3, and most modern LLMs\n",
    "    \"\"\")"
   ],
   "id": "c33dc027380fe632",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRANSFORMER BLOCK COMPARISON\n",
      "======================================================================\n",
      "\n",
      "1. ORIGINAL TRANSFORMER (Post-Norm, ReLU)\n",
      "----------------------------------------------------------------------\n",
      "Input shape:  torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Parameters:   3,152,384\n",
      "Architecture: X -> MHA -> Add -> Norm -> FFN -> Add -> Norm\n",
      "\n",
      "2. MODERN TRANSFORMER (Pre-Norm, GELU)\n",
      "----------------------------------------------------------------------\n",
      "Input shape:  torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Parameters:   3,152,384\n",
      "Architecture: X -> Norm -> MHA -> Add -> Norm -> FFN -> Add\n",
      "\n",
      "======================================================================\n",
      "KEY DIFFERENCES\n",
      "======================================================================\n",
      "\n",
      "Original (Post-Norm):\n",
      "  ✓ Residual connection first, then normalization\n",
      "  ✓ ReLU activation in FFN\n",
      "  ✓ Separate Q, K, V projections\n",
      "  ✗ Can have training instability with deep networks\n",
      "  ✗ Requires careful learning rate tuning\n",
      "\n",
      "Modern (Pre-Norm):\n",
      "  ✓ Normalization first, then residual connection\n",
      "  ✓ GELU activation (smoother gradients)\n",
      "  ✓ Fused QKV projection (more efficient)\n",
      "  ✓ Better gradient flow, easier to train deep networks\n",
      "  ✓ More stable training, works well with larger learning rates\n",
      "  ✓ Used in GPT-2, GPT-3, and most modern LLMs\n",
      "    \n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
