{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-03T23:57:35.608949Z",
     "start_time": "2025-10-03T23:57:33.186958Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Tuple"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T23:57:35.624373Z",
     "start_time": "2025-10-03T23:57:35.619273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class KVCache:\n",
    "    \"\"\"\n",
    "    Key-Value cache for transformer attention mechanism.\n",
    "    Stores past key and value states to avoid recomputation during autoregressive generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size: int, max_seq_len: int, num_heads: int, head_dim: int, device: str = 'cpu'):\n",
    "        \"\"\"\n",
    "        Initialize KV cache.\n",
    "\n",
    "        Args:\n",
    "            batch_size: Batch size\n",
    "            max_seq_len: Maximum sequence length to cache\n",
    "            num_heads: Number of attention heads\n",
    "            head_dim: Dimension of each attention head\n",
    "            device: Device to store cache on ('cpu' or 'cuda')\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize cache tensors\n",
    "        self.k_cache = torch.zeros(\n",
    "            batch_size, num_heads, max_seq_len, head_dim,\n",
    "            dtype=torch.float32, device=device\n",
    "        )\n",
    "        self.v_cache = torch.zeros(\n",
    "            batch_size, num_heads, max_seq_len, head_dim,\n",
    "            dtype=torch.float32, device=device\n",
    "        )\n",
    "\n",
    "        # Track current position in cache\n",
    "        self.current_len = 0\n",
    "\n",
    "    def update(self, k: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Update cache with new key and value tensors.\n",
    "\n",
    "        Args:\n",
    "            k: New keys of shape (batch_size, num_heads, seq_len, head_dim)\n",
    "            V: New values of shape (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (all_keys, all_values) including cached and new states\n",
    "        \"\"\"\n",
    "        seq_len = k.shape[2]\n",
    "\n",
    "        # Check if we need to extend cache\n",
    "        if self.current_len + seq_len > self.max_seq_len:\n",
    "            raise ValueError(f\"Cache overflow: current_len={self.current_len}, new_seq_len={seq_len}, max_seq_len={self.max_seq_len}\")\n",
    "\n",
    "        # Update cache at current position\n",
    "        self.k_cache[:, :, self.current_len:self.current_len + seq_len, :] = k\n",
    "        self.v_cache[:, :, self.current_len:self.current_len + seq_len, :] = v\n",
    "\n",
    "        # Increment position\n",
    "        self.current_len += seq_len\n",
    "\n",
    "        # Return all cached keys and values up to current position\n",
    "        return (\n",
    "            self.k_cache[:, :, :self.current_len, :],\n",
    "            self.v_cache[:, :, :self.current_len, :],\n",
    "        )\n",
    "\n",
    "    def get(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get current cached keys and values.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (keys, values) from cache\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.k_cache[:, :, :self.current_len, :],\n",
    "            self.v_cache[:, :, :self.current_len, :],\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset cache to empty state.\"\"\"\n",
    "        self.current_len = 0\n",
    "        self.k_cache.zero_()\n",
    "        self.v_cache.zero_()\n",
    "\n",
    "    def get_seq_len(self) -> int:\n",
    "        \"\"\"Get current sequence length in cache.\"\"\"\n",
    "        return self.current_len"
   ],
   "id": "454398f8ff3ab92d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T23:57:35.633835Z",
     "start_time": "2025-10-03T23:57:35.628964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttentionWithCache(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention with KV cache support for efficient autoregressive generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        # Linear projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, kv_cache: Optional[KVCache] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with optional KV cache.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            kv_cache: Optional KV cache for autoregressive generation\n",
    "            mask: Optional attention mask\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        B, L, D = x.shape\n",
    "\n",
    "        # Project queries, keys, values\n",
    "        q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Use cache if provided\n",
    "        if kv_cache is not None:\n",
    "            k, v = kv_cache.update(k, v)\n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax and dropout\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        # Reshape and project output\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, D)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out"
   ],
   "id": "56dd7a2b99629e4c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T23:57:35.711038Z",
     "start_time": "2025-10-03T23:57:35.637475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    max_seq_len = 100\n",
    "\n",
    "    # Create attention layer\n",
    "    attention = MultiHeadAttentionWithCache(d_model, num_heads)\n",
    "\n",
    "    # Create KV cache\n",
    "    head_dim = d_model // num_heads\n",
    "    kv_cache = KVCache(batch_size, max_seq_len, num_heads, head_dim)\n",
    "\n",
    "    print(\"=== Autoregressive Generation with KV Cache ===\\n\")\n",
    "\n",
    "    # Simulate autoregressive generation\n",
    "    for step in range(3):\n",
    "        # In generation, we typically process one token at a time\n",
    "        token_len = 1 if step > 0 else seq_len  # First step: full sequence, then one token\n",
    "        x = torch.randn(batch_size, token_len, d_model)\n",
    "\n",
    "        print(f\"Step {step + 1}:\")\n",
    "        print(f\"  Input shape: {x.shape}\")\n",
    "        print(f\"  Cache length before: {kv_cache.get_seq_len()}\")\n",
    "\n",
    "        # Forward pass with cache\n",
    "        output = attention(x, kv_cache=kv_cache)\n",
    "\n",
    "        print(f\"  Cache length after: {kv_cache.get_seq_len()}\")\n",
    "        print(f\"  Output shape: {output.shape}\\n\")\n",
    "\n",
    "    # Reset cache\n",
    "    print(\"Resetting cache...\")\n",
    "    kv_cache.reset()\n",
    "    print(f\"Cache length after reset: {kv_cache.get_seq_len()}\\n\")\n",
    "\n",
    "    # Compare with and without cache (timing)\n",
    "    print(\"=== Performance Comparison ===\")\n",
    "    import time\n",
    "\n",
    "    # Without cache\n",
    "    x_full = torch.randn(batch_size, 50, d_model)\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = attention(x_full, kv_cache=None)\n",
    "    no_cache_time = time.time() - start\n",
    "    print(f\"Without cache (10 iterations): {no_cache_time:.4f}s\")\n",
    "\n",
    "    # With cache (simulating generation)\n",
    "    kv_cache.reset()\n",
    "    start = time.time()\n",
    "    for i in range(10):\n",
    "        x_token = torch.randn(batch_size, 1, d_model)\n",
    "        _ = attention(x_token, kv_cache=kv_cache)\n",
    "    cache_time = time.time() - start\n",
    "    print(f\"With cache (10 tokens): {cache_time:.4f}s\")\n",
    "    print(f\"Speedup: {no_cache_time / cache_time:.2f}x\")"
   ],
   "id": "9405d99507d12047",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Autoregressive Generation with KV Cache ===\n",
      "\n",
      "Step 1:\n",
      "  Input shape: torch.Size([2, 10, 512])\n",
      "  Cache length before: 0\n",
      "  Cache length after: 10\n",
      "  Output shape: torch.Size([2, 10, 512])\n",
      "\n",
      "Step 2:\n",
      "  Input shape: torch.Size([2, 1, 512])\n",
      "  Cache length before: 10\n",
      "  Cache length after: 11\n",
      "  Output shape: torch.Size([2, 1, 512])\n",
      "\n",
      "Step 3:\n",
      "  Input shape: torch.Size([2, 1, 512])\n",
      "  Cache length before: 11\n",
      "  Cache length after: 12\n",
      "  Output shape: torch.Size([2, 1, 512])\n",
      "\n",
      "Resetting cache...\n",
      "Cache length after reset: 0\n",
      "\n",
      "=== Performance Comparison ===\n",
      "Without cache (10 iterations): 0.0082s\n",
      "With cache (10 tokens): 0.0029s\n",
      "Speedup: 2.87x\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
