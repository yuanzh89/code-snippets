{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-23T06:15:15.355864Z",
     "start_time": "2025-09-23T06:15:15.350471Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention implementation\n",
    "\n",
    "        Args:\n",
    "            d_model: Model dimension (embedding size)\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0, \"d-model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        # Dimension of each head\n",
    "        # Splits embedding dimension into multiple smaller chunks\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=True)\n",
    "\n",
    "        # Output projection\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Compute scaled dot product attention\n",
    "\n",
    "        Args:\n",
    "            Q, K, V: Query, Key, Value matrices\n",
    "            mask: Optional attention mask\n",
    "\n",
    "        Returns:\n",
    "            attention_output: Weighted values\n",
    "            attention_weights: Attention weights\n",
    "        \"\"\"\n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply mask if provided (set masked positions to large negative value.\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return attention_output, attention_weights\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention\n",
    "\n",
    "        Args:\n",
    "            query: Query tensor [batch_size, seq_len, d_model]\n",
    "            key: Key tensor [batch_size, seq_len, d_model]\n",
    "            value: Value tensor [batch_size, seq_len, d_model]\n",
    "            mask: Optional attention mask [batch_size, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output: Attention output [batch_size, seq_len, d_model]\n",
    "            attention_weights: Attention weights for visualization\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = query.size(0), query.size(1)\n",
    "\n",
    "        # 1. Linear projections to get Q, K, V\n",
    "        Q = self.w_q(query)  # [batch_size, seq_len, d_model]\n",
    "        K = self.w_k(key)    # [batch_size, seq_len, d_model]\n",
    "        V = self.w_v(value)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # 2. Reshape and transpose for multi-head attention\n",
    "        # Split into multiple heads: [batch_size, num_heads, seq_len, d_k]\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 3. Apply scaled dot-product attention\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Concatenate heads\n",
    "        # Transpose back: [batch_size, seq_len, num_heads, d_k]\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        # Reshape to concatenate heads: [batch_size, seq_len, d_model]\n",
    "        attention_output = attention_output.view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # 5. Final linear projection\n",
    "        output = self.w_o(attention_output)\n",
    "\n",
    "        return output, attention_weights"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T06:15:15.368514Z",
     "start_time": "2025-09-23T06:15:15.358793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage and testing\n",
    "def create_padding_mask(seq, pad_token=0):\n",
    "    \"\"\"Create padding mask for sequences with padding tokens\"\"\"\n",
    "    return (seq != pad_token).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Create causal (lower triangular) mask for decoder self-attention\"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "\n",
    "    # Create sample input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Initialize multi-head attention\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    # Self-attention (query, key, value are the same)\n",
    "    output, attention_weights = mha(x, x, x)\n",
    "\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "\n",
    "    # Example with causal mask (for decoder)\n",
    "    causal_mask = create_causal_mask(seq_len)\n",
    "    output_masked, attention_weights_masked = mha(x, x, x, mask=causal_mask)\n",
    "\n",
    "    print(f\"\\nWith causal mask:\")\n",
    "    print(f\"Output shape: {output_masked.shape}\")\n",
    "    print(f\"Attention weights shape: {attention_weights_masked.shape}\")\n",
    "\n",
    "    # Verify that future positions are masked (should be close to 0)\n",
    "    print(f\"Attention to future positions (should be ~0): {attention_weights_masked[0, 0, 0, -1].item():.6f}\")\n",
    "    print(f\"Attention to current/past positions: {attention_weights_masked[0, 0, 0, 0].item():.6f}\")"
   ],
   "id": "ec904c93559927a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n",
      "\n",
      "With causal mask:\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n",
      "Attention to future positions (should be ~0): 0.138889\n",
      "Attention to current/past positions: 0.089260\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T06:15:15.378637Z",
     "start_time": "2025-09-23T06:15:15.375117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Complete Transformer Block with Multi-Head Attention\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Complete Transformer block with multi-head attention and feed-forward network\n",
    "\n",
    "        Args:\n",
    "            d_model: Model dimension (embedding size)\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Feed-forward network dimension\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, attention_weights = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return x, attention_weights"
   ],
   "id": "973ab94f59a78433",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T06:15:15.396849Z",
     "start_time": "2025-09-23T06:15:15.382722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example with complete transformer block\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPLETE TRANSFORMER BLOCK EXAMPLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "transformer_block = TransformerBlock(d_model=512, num_heads=8, d_ff=2048)\n",
    "output, attention_weights = transformer_block(x)\n",
    "\n",
    "print(f\"Transformer block output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ],
   "id": "c140c845cf8e92bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "COMPLETE TRANSFORMER BLOCK EXAMPLE\n",
      "==================================================\n",
      "Transformer block output shape: torch.Size([2, 10, 512])\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
