{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-30T21:20:09.293984Z",
     "start_time": "2025-09-30T21:20:09.288449Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention implementation\n",
    "\n",
    "        Args:\n",
    "            d_model: Model dimension (embedding size)\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        # Dimension of each head\n",
    "        # Splits embedding dimension into multiple smaller chunks\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=True)\n",
    "\n",
    "        # Output projection\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Compute scaled dot product attention\n",
    "\n",
    "        Args:\n",
    "            Q, K, V: Query, Key, Value matrices\n",
    "            mask: Optional attention mask\n",
    "\n",
    "        Returns:\n",
    "            attention_output: Weighted values\n",
    "            attention_weights: Attention weights\n",
    "        \"\"\"\n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply mask if provided (set masked positions to large negative value.\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return attention_output, attention_weights\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention\n",
    "\n",
    "        Args:\n",
    "            query: Query tensor [batch_size, seq_len, d_model]\n",
    "            key: Key tensor [batch_size, seq_len, d_model]\n",
    "            value: Value tensor [batch_size, seq_len, d_model]\n",
    "            mask: Optional attention mask [batch_size, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output: Attention output [batch_size, seq_len, d_model]\n",
    "            attention_weights: Attention weights for visualization\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = query.size(0), query.size(1)\n",
    "\n",
    "        # 1. Linear projections to get Q, K, V\n",
    "        Q = self.w_q(query)  # [batch_size, seq_len, d_model]\n",
    "        K = self.w_k(key)    # [batch_size, seq_len, d_model]\n",
    "        V = self.w_v(value)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # 2. Reshape and transpose for multi-head attention\n",
    "        # Split into multiple heads: [batch_size, num_heads, seq_len, d_k]\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 3. Apply scaled dot-product attention\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Concatenate heads\n",
    "        # Transpose back: [batch_size, seq_len, num_heads, d_k]\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        # Reshape to concatenate heads: [batch_size, seq_len, d_model]\n",
    "        attention_output = attention_output.view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # 5. Final linear projection\n",
    "        output = self.w_o(attention_output)\n",
    "\n",
    "        return output, attention_weights"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:20:09.309087Z",
     "start_time": "2025-09-30T21:20:09.300004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage and testing\n",
    "def create_padding_mask(seq, pad_token=0):\n",
    "    \"\"\"Create padding mask for sequences with padding tokens\"\"\"\n",
    "    return (seq != pad_token).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Create causal (lower triangular) mask for decoder self-attention\"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "\n",
    "    # Create sample input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Initialize multi-head attention\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    # Self-attention (query, key, value are the same)\n",
    "    output, attention_weights = mha(x, x, x)\n",
    "\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "\n",
    "    # Example with causal mask (for decoder)\n",
    "    causal_mask = create_causal_mask(seq_len)\n",
    "    output_masked, attention_weights_masked = mha(x, x, x, mask=causal_mask)\n",
    "\n",
    "    print(f\"\\nWith causal mask:\")\n",
    "    print(f\"Output shape: {output_masked.shape}\")\n",
    "    print(f\"Attention weights shape: {attention_weights_masked.shape}\")\n",
    "\n",
    "    # Verify that future positions are masked (should be close to 0)\n",
    "    print(f\"Attention to future positions (should be ~0): {attention_weights_masked[0, 0, 0, -1].item():.6f}\")\n",
    "    print(f\"Attention to current/past positions: {attention_weights_masked[0, 0, 0, 0].item():.6f}\")"
   ],
   "id": "ec904c93559927a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n",
      "\n",
      "With causal mask:\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n",
      "Attention to future positions (should be ~0): 0.000000\n",
      "Attention to current/past positions: 1.111111\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:20:09.318334Z",
     "start_time": "2025-09-30T21:20:09.314955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Complete Transformer Block with Multi-Head Attention\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Complete Transformer block with multi-head attention and feed-forward network\n",
    "\n",
    "        Args:\n",
    "            d_model: Model dimension (embedding size)\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Feed-forward network dimension\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, attention_weights = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return x, attention_weights"
   ],
   "id": "973ab94f59a78433",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:20:09.349907Z",
     "start_time": "2025-09-30T21:20:09.331208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example with complete transformer block\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPLETE TRANSFORMER BLOCK EXAMPLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "transformer_block = TransformerBlock(d_model=512, num_heads=8, d_ff=2048)\n",
    "output, attention_weights = transformer_block(x)\n",
    "\n",
    "print(f\"Transformer block output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ],
   "id": "c140c845cf8e92bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "COMPLETE TRANSFORMER BLOCK EXAMPLE\n",
      "==================================================\n",
      "Transformer block output shape: torch.Size([2, 10, 512])\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:20:09.357403Z",
     "start_time": "2025-09-30T21:20:09.353706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# POST-NORM TRANSFORMER BLOCK (Original Transformer - \"Attention Is All You Need\")\n",
    "# ============================================================================\n",
    "class PostNormTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    POST-NORM: LayerNorm is applied AFTER the residual connection\n",
    "    Pattern: x = LayerNorm(x + Sublayer(x))\n",
    "\n",
    "    Pros:\n",
    "    - Original Transformer architecture\n",
    "    - Stronger gradient signal to early layers\n",
    "\n",
    "    Cons:\n",
    "    - Can be unstable during training\n",
    "    - Often requires learning rate warmup\n",
    "    - Harder to train very deep models\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(PostNormTransformerBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # POST-NORM: Norm AFTER residual\n",
    "\n",
    "        # Attention block\n",
    "        attention_output, attention_weights = self.attention(x, x, x, mask)\n",
    "        # Norm after addition\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "\n",
    "        # Feed-forward block\n",
    "        ff_output = self.ff(x)\n",
    "        # Norm after addition\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return x, attention_weights"
   ],
   "id": "111fa2129beda65d",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:20:09.370198Z",
     "start_time": "2025-09-30T21:20:09.366632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# PRE-NORM TRANSFORMER BLOCK (Modern variant - GPT, BERT variants)\n",
    "# ============================================================================\n",
    "class PreNormTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    PRE-NORM: LayerNorm is applied BEFORE the sub-layer (attention or FFN)\n",
    "    Pattern: x = x + Sublayer(LayerNorm(x))\n",
    "\n",
    "    Pros:\n",
    "    - More stable training\n",
    "    - Easier to train deep models (100+ layers)\n",
    "    - No need for learning rate warmup\n",
    "    - Better gradient flow\n",
    "\n",
    "    Cons:\n",
    "    - Slightly different from original paper\n",
    "    - May need additional final LayerNorm at the end of the model\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(PreNormTransformerBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # PRE-NORM: Norm BEFORE sub-layer\n",
    "\n",
    "        # Attention block\n",
    "        normed_x = self.norm1(x)  # Norm first\n",
    "        attention_output, attention_weights = self.attention(normed_x, normed_x, normed_x, mask)\n",
    "        x = x + self.dropout(attention_output)  # Residual without norm\n",
    "\n",
    "        # Feed-forward block\n",
    "        normed_x = self.norm2(x)  # Norm first\n",
    "        ff_output = self.ff(normed_x)\n",
    "        x = x + self.dropout(ff_output)  # Residual without norm\n",
    "\n",
    "        return x, attention_weights"
   ],
   "id": "d04dcf61ce399ece",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:20:09.412168Z",
     "start_time": "2025-09-30T21:20:09.378773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# DEMONSTRATION AND COMPARISON\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"POST-NORM vs PRE-NORM TRANSFORMER BLOCKS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Parameters\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "\n",
    "    # Create sample input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Initialize both versions\n",
    "    post_norm_block = PostNormTransformerBlock(d_model, num_heads, d_ff)\n",
    "    pre_norm_block = PreNormTransformerBlock(d_model, num_heads, d_ff)\n",
    "\n",
    "    print(\"\\n--- POST-NORM (Original Transformer) ---\")\n",
    "    print(\"Order: Sublayer → Add Residual → LayerNorm\")\n",
    "    output_post, _ = post_norm_block(x)\n",
    "    print(f\"Input shape:  {x.shape}\")\n",
    "    print(f\"Output shape: {output_post.shape}\")\n",
    "    print(f\"Output mean:  {output_post.mean().item():.6f}\")\n",
    "    print(f\"Output std:   {output_post.std().item():.6f}\")\n",
    "\n",
    "    print(\"\\n--- PRE-NORM (Modern variant) ---\")\n",
    "    print(\"Order: LayerNorm → Sublayer → Add Residual\")\n",
    "    output_pre, _ = pre_norm_block(x)\n",
    "    print(f\"Input shape:  {x.shape}\")\n",
    "    print(f\"Output shape: {output_pre.shape}\")\n",
    "    print(f\"Output mean:  {output_pre.mean().item():.6f}\")\n",
    "    print(f\"Output std:   {output_pre.std().item():.6f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KEY DIFFERENCES\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\"\"\n",
    "POST-NORM (x = LayerNorm(x + Sublayer(x))):\n",
    "  ✓ Original Transformer architecture\n",
    "  ✓ Stronger gradient signal\n",
    "  ✗ Can be unstable with deep models\n",
    "  ✗ Requires learning rate warmup\n",
    "\n",
    "PRE-NORM (x = x + Sublayer(LayerNorm(x))):\n",
    "  ✓ More stable training\n",
    "  ✓ Better for very deep models (100+ layers)\n",
    "  ✓ No warmup needed\n",
    "  ✓ Used in GPT-2, GPT-3, many modern models\n",
    "  ✗ May need final LayerNorm at end of model\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"WHEN TO USE WHICH?\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\"\"\n",
    "Use POST-NORM when:\n",
    "  - Following the original Transformer paper exactly\n",
    "  - Building shallow models (6-12 layers)\n",
    "  - You have good hyperparameter tuning infrastructure\n",
    "\n",
    "Use PRE-NORM when:\n",
    "  - Building deep models (20+ layers)\n",
    "  - You want stable training out-of-the-box\n",
    "  - You want faster convergence\n",
    "  - Building large language models (like GPT)\n",
    "    Sequential\"\"\")"
   ],
   "id": "8d08627711229474",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "POST-NORM vs PRE-NORM TRANSFORMER BLOCKS\n",
      "======================================================================\n",
      "\n",
      "--- POST-NORM (Original Transformer) ---\n",
      "Order: Sublayer → Add Residual → LayerNorm\n",
      "Input shape:  torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Output mean:  -0.000000\n",
      "Output std:   1.000044\n",
      "\n",
      "--- PRE-NORM (Modern variant) ---\n",
      "Order: LayerNorm → Sublayer → Add Residual\n",
      "Input shape:  torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Output mean:  -0.000116\n",
      "Output std:   1.060625\n",
      "\n",
      "======================================================================\n",
      "KEY DIFFERENCES\n",
      "======================================================================\n",
      "\n",
      "POST-NORM (x = LayerNorm(x + Sublayer(x))):\n",
      "  ✓ Original Transformer architecture\n",
      "  ✓ Stronger gradient signal\n",
      "  ✗ Can be unstable with deep models\n",
      "  ✗ Requires learning rate warmup\n",
      "\n",
      "PRE-NORM (x = x + Sublayer(LayerNorm(x))):\n",
      "  ✓ More stable training\n",
      "  ✓ Better for very deep models (100+ layers)\n",
      "  ✓ No warmup needed\n",
      "  ✓ Used in GPT-2, GPT-3, many modern models\n",
      "  ✗ May need final LayerNorm at end of model\n",
      "    \n",
      "\n",
      "======================================================================\n",
      "WHEN TO USE WHICH?\n",
      "======================================================================\n",
      "\n",
      "Use POST-NORM when:\n",
      "  - Following the original Transformer paper exactly\n",
      "  - Building shallow models (6-12 layers)\n",
      "  - You have good hyperparameter tuning infrastructure\n",
      "\n",
      "Use PRE-NORM when:\n",
      "  - Building deep models (20+ layers)\n",
      "  - You want stable training out-of-the-box\n",
      "  - You want faster convergence\n",
      "  - Building large language models (like GPT)\n",
      "    Sequential\n"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
