{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-03T06:11:58.569279Z",
     "start_time": "2025-10-03T06:11:57.530458Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:11:58.575682Z",
     "start_time": "2025-10-03T06:11:58.573253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def standard_attention(Q, K, V, mask=None):\n",
    "    \"\"\"Standard attention implementation for comparison.\"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "\n",
    "    scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "\n",
    "    return output"
   ],
   "id": "8d98e8c1ec5caa43",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:11:58.586060Z",
     "start_time": "2025-10-03T06:11:58.578553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flash_attention_forward(Q, K, V, block_size=64):\n",
    "    \"\"\"\n",
    "    Simplified Flash Attention implementation.\n",
    "\n",
    "    Args:\n",
    "        Q, K, V: Query, Key, Value matrices [batch, num_heads, seq_len, head_dim]\n",
    "        block_size: Size of blocks for tiling (should fit in SRAM)\n",
    "\n",
    "    Returns:\n",
    "        output: Attention output [batch, num_heads, seq_len, head_dim]\n",
    "    \"\"\"\n",
    "    batch_size, num_heads, seq_len, head_dim = Q.shape\n",
    "    scale = 1.0 / math.sqrt(head_dim)\n",
    "\n",
    "    # Initialize output and normalization statistics\n",
    "    O = torch.zeros_like(Q)\n",
    "    l = torch.zeros(batch_size, num_heads, seq_len, 1, device=Q.device)  # row sum\n",
    "    m = torch.full((batch_size, num_heads, seq_len, 1), float('-inf'), device=Q.device)  # row max\n",
    "\n",
    "    # Number of blocks\n",
    "    num_blocks = (seq_len + block_size - 1) // block_size\n",
    "\n",
    "    # Outer loop: iterate over query blocks\n",
    "    for i in range(num_blocks):\n",
    "        q_start = i * block_size\n",
    "        q_end = min((i + 1) * block_size, seq_len)\n",
    "        Q_block = Q[:, :, q_start:q_end, :]  # [batch, num_heads, block_size, D]\n",
    "\n",
    "        # Initialize block outputs\n",
    "        O_block = torch.zeros_like(Q_block)\n",
    "        l_block = torch.zeros(batch_size, num_heads, q_end - q_start, 1, device=Q.device)\n",
    "        m_block = torch.full((batch_size, num_heads, q_end - q_start, 1), float('-inf'), device=Q.device)\n",
    "\n",
    "        # Inner loop: iterate over key-value blocks\n",
    "        for j in range(num_blocks):\n",
    "            k_start = j * block_size\n",
    "            k_end = min((j + 1) * block_size, seq_len)\n",
    "            K_block = K[:, :, k_start:k_end, :]  # [B, H, block_size, D]\n",
    "            V_block = V[:, :, k_start:k_end, :]  # [B, H, block_size, D]\n",
    "\n",
    "            # Compute attention scores for this block\n",
    "            S_block = torch.matmul(Q_block, K_block.transpose(-1, -2)) * scale  # [B, H, q_block, k_block]\n",
    "\n",
    "            # Online softmax: update running statistics\n",
    "            m_new = torch.maximum(m_block, S_block.max(dim=-1, keepdim=True)[0])\n",
    "\n",
    "            # Compute exponentials with numerical stability\n",
    "            exp_scores = torch.exp(S_block - m_new)\n",
    "            exp_m_diff = torch.exp(m_block - m_new)\n",
    "\n",
    "            # Update row sum\n",
    "            l_new = exp_m_diff * l_block + exp_scores.sum(dim=-1, keepdim=True)\n",
    "\n",
    "            # Update output (weighted combination)\n",
    "            O_block = exp_m_diff * O_block + torch.matmul(exp_scores, V_block)\n",
    "\n",
    "            # Update statistics for next iteration\n",
    "            m_block = m_new\n",
    "            l_block = l_new\n",
    "\n",
    "        # Normalize the block output\n",
    "        O[:, :, q_start:q_end, :] = O_block / l_block\n",
    "        m[:, :, q_start:q_end, :] = m_block\n",
    "        l[:, :, q_start:q_end, :] = l_block\n",
    "\n",
    "    return O"
   ],
   "id": "c904d47babb1477e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:11:58.595167Z",
     "start_time": "2025-10-03T06:11:58.588995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flash_attention_causal(Q, K, V, block_size=64):\n",
    "    \"\"\"\n",
    "    Flash Attention with causal masking (for autoregressive models)\n",
    "\n",
    "    Args:\n",
    "        Q, K, V: Query, Key, Value matrices [batch, num_heads, seq_len, head_dim]\n",
    "        block_size: Size of blocks for tiling (should fit in SRAM)\n",
    "\n",
    "    Returns:\n",
    "        output: Attention output with causal masking\n",
    "    \"\"\"\n",
    "    batch_size, num_heads, seq_len, head_dim = Q.shape\n",
    "    scale = 1.0 / math.sqrt(head_dim)\n",
    "\n",
    "    O = torch.zeros_like(Q)\n",
    "    l = torch.zeros(batch_size, num_heads, seq_len, 1, device=Q.device)\n",
    "    m = torch.full((batch_size, num_heads, seq_len, 1), float('-inf'), device=Q.device)\n",
    "\n",
    "    num_blocks = (seq_len + block_size - 1) // block_size\n",
    "\n",
    "    for i in range(num_blocks):\n",
    "        q_start = i * block_size\n",
    "        q_end = min((i + 1) * block_size, seq_len)\n",
    "        Q_block = Q[:, :, q_start:q_end, :]\n",
    "\n",
    "        O_block = torch.zeros_like(Q_block)\n",
    "        l_block = torch.zeros(batch_size, num_heads, q_end - q_start, 1, device=Q.device)\n",
    "        m_block = torch.full((batch_size, num_heads, q_end - q_start, 1), float('-inf'), device=Q.device)\n",
    "\n",
    "        # Only attend to previous blocks (causal)\n",
    "        for j in range(i + 1):  # j <= i ensures causality\n",
    "            k_start = j * block_size\n",
    "            k_end = min((j + 1) * block_size, seq_len)\n",
    "            K_block = K[:, :, k_start:k_end, :]\n",
    "            V_block = V[:, :, k_start:k_end, :]\n",
    "\n",
    "            S_block = torch.matmul(Q_block, K_block.transpose(-1, -2)) * scale\n",
    "\n",
    "            # Apply causal mask within block\n",
    "            if i == j:  # Same block - need causal mask\n",
    "                q_indices = torch.arange(q_start, q_end, device=Q.device).unsqueeze(1)\n",
    "                k_indices = torch.arange(k_start, k_end, device=Q.device).unsqueeze(0)\n",
    "                causal_mask = q_indices >= k_indices\n",
    "                S_block = S_block.masked_fill(~causal_mask, float('-inf'))\n",
    "\n",
    "            m_new = torch.maximum(m_block, S_block.max(dim=-1, keepdim=True)[0])\n",
    "            exp_scores = torch.exp(S_block - m_new)\n",
    "            exp_m_diff = torch.exp(m_block - m_new)\n",
    "\n",
    "            l_new = exp_m_diff * l_block + exp_scores.sum(dim=-1, keepdim=True)\n",
    "            O_block = exp_m_diff * O_block + torch.matmul(exp_scores, V_block)\n",
    "\n",
    "            m_block = m_new\n",
    "            l_block = l_new\n",
    "\n",
    "        O[:, :, q_start:q_end, :] = O_block / l_block\n",
    "        m[:, :, q_start:q_end, :] = m_block\n",
    "        l[:, :, q_start:q_end, :] = l_block\n",
    "\n",
    "    return O"
   ],
   "id": "7a0e2bd6eeb5f301",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:11:58.887919Z",
     "start_time": "2025-10-03T06:11:58.598763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage and verification\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup\n",
    "    batch_size = 2\n",
    "    num_heads = 8\n",
    "    seq_len = 128\n",
    "    head_dim = 64\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    Q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    K = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    V = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Compare standard vs flash attention\n",
    "    output_standard = standard_attention(Q, K, V)\n",
    "    output_flash = flash_attention_forward(Q, K, V, block_size=32)\n",
    "\n",
    "    # Check if outputs match (they should be very close)\n",
    "    diff = (output_standard - output_flash).abs().max()\n",
    "    print(f\"Max difference between standard and flash attention: {diff.item():.6e}\")\n",
    "    print(f\"Outputs match: {torch.allclose(output_standard, output_flash, atol=1e-4)}\")\n",
    "\n",
    "    # Test causal version\n",
    "    output_flash_causal = flash_attention_causal(Q, K, V, block_size=32)\n",
    "\n",
    "    # Create causal mask for standard attention\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=Q.device)).unsqueeze(0).unsqueeze(0)\n",
    "    output_standard_causal = standard_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "    diff_causal = (output_standard_causal - output_flash_causal).abs().max()\n",
    "    print(f\"\\nMax difference for causal attention: {diff_causal.item():.6e}\")\n",
    "    print(f\"Causal outputs match: {torch.allclose(output_standard_causal, output_flash_causal, atol=1e-4)}\")\n",
    "\n",
    "    print(f\"\\nInput shape: {Q.shape}\")\n",
    "    print(f\"Output shape: {output_flash.shape}\")"
   ],
   "id": "5ceeb54fce33d5e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference between standard and flash attention: 4.768372e-07\n",
      "Outputs match: True\n",
      "\n",
      "Max difference for causal attention: 4.768372e-07\n",
      "Causal outputs match: True\n",
      "\n",
      "Input shape: torch.Size([2, 8, 128, 64])\n",
      "Output shape: torch.Size([2, 8, 128, 64])\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
